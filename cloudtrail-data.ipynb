{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'conf': {'spark.pyspark.python': 'python3', 'spark.pyspark.virtualenv.enabled': 'true', 'spark.pyspark.virtualenv.type': 'native', 'spark.pyspark.virtualenv.bin.path': '/usr/bin/virtualenv'}, 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "No active sessions."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'conf': {'spark.jars.packages': 'org.elasticsearch:elasticsearch-hadoop:7.10.1', 'spark.pyspark.python': 'python3', 'spark.pyspark.virtualenv.enabled': 'true', 'spark.pyspark.virtualenv.type': 'native', 'spark.pyspark.virtualenv.bin.path': '/usr/bin/virtualenv'}, 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "No active sessions."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure -f\n",
    "{\n",
    "    \"conf\": {\n",
    "        \"spark.jars.packages\": \"org.elasticsearch:elasticsearch-hadoop:7.10.1\",\n",
    "        \"spark.pyspark.python\": \"python3\",\n",
    "        \"spark.pyspark.virtualenv.enabled\": \"true\",\n",
    "        \"spark.pyspark.virtualenv.type\": \"native\",\n",
    "        \"spark.pyspark.virtualenv.bin.path\": \"/usr/bin/virtualenv\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abf3d1f3e4564619b4553ee12e9253ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>39</td><td>application_1606748823821_0038</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-81-0.ec2.internal:20888/proxy/application_1606748823821_0038/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-95-66.ec2.internal:8042/node/containerlogs/container_1606748823821_0038_01_000001/livy\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package                    Version  \n",
      "-------------------------- ---------\n",
      "beautifulsoup4             4.9.1    \n",
      "boto                       2.49.0   \n",
      "click                      7.1.2    \n",
      "jmespath                   0.10.0   \n",
      "joblib                     0.16.0   \n",
      "lxml                       4.5.2    \n",
      "mysqlclient                1.4.2    \n",
      "nltk                       3.5      \n",
      "nose                       1.3.4    \n",
      "numpy                      1.16.5   \n",
      "pip                        9.0.1    \n",
      "py-dateutil                2.2      \n",
      "python37-sagemaker-pyspark 1.4.0    \n",
      "pytz                       2020.1   \n",
      "PyYAML                     5.3.1    \n",
      "regex                      2020.7.14\n",
      "setuptools                 28.8.0   \n",
      "six                        1.13.0   \n",
      "soupsieve                  1.9.5    \n",
      "tqdm                       4.48.2   \n",
      "wheel                      0.29.0   \n",
      "windmill                   1.6"
     ]
    }
   ],
   "source": [
    "sc.list_packages()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Open a bucket of cloudtrail stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fca558ff18f34cb1b0ed08a740f2d958",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cloudTrailLogsPath = \"s3n://cloudtrail-awslogs-994949494599-hhzedb7l-isengard-do-not-delete/AWSLogs/*/CloudTrail/*/*/*/*/\""
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Here I will define the schema for the CloudTrail events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4169eb9fd03420bbf6cb6ab474b9109",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "# from pyspark.sql.streaming import ProcessingTime\n",
    "from pyspark.sql.types import *\n",
    "from datetime import datetime\n",
    "\n",
    "cloudTrailSchema = StructType() \\\n",
    "  .add(\"Records\", ArrayType(StructType() \\\n",
    "    .add(\"additionalEventData\", StringType()) \\\n",
    "    .add(\"apiVersion\", StringType()) \\\n",
    "    .add(\"awsRegion\", StringType()) \\\n",
    "    .add(\"errorCode\", StringType()) \\\n",
    "    .add(\"errorMessage\", StringType()) \\\n",
    "    .add(\"eventID\", StringType()) \\\n",
    "    .add(\"eventName\", StringType()) \\\n",
    "    .add(\"eventSource\", StringType()) \\\n",
    "    .add(\"eventTime\", StringType()) \\\n",
    "    .add(\"eventType\", StringType()) \\\n",
    "    .add(\"eventVersion\", StringType()) \\\n",
    "    .add(\"readOnly\", BooleanType()) \\\n",
    "    .add(\"recipientAccountId\", StringType()) \\\n",
    "    .add(\"requestID\", StringType()) \\\n",
    "    .add(\"requestParameters\", MapType(StringType(), StringType())) \\\n",
    "    .add(\"resources\", ArrayType(StructType() \\\n",
    "      .add(\"ARN\", StringType()) \\\n",
    "      .add(\"accountId\", StringType()) \\\n",
    "      .add(\"type\", StringType()) \\\n",
    "    )) \\\n",
    "    .add(\"responseElements\", MapType(StringType(), StringType())) \\\n",
    "    .add(\"sharedEventID\", StringType()) \\\n",
    "    .add(\"sourceIPAddress\", StringType()) \\\n",
    "    .add(\"serviceEventDetails\", MapType(StringType(), StringType())) \\\n",
    "    .add(\"userAgent\", StringType()) \\\n",
    "    .add(\"userIdentity\", StructType() \\\n",
    "      .add(\"accessKeyId\", StringType()) \\\n",
    "      .add(\"accountId\", StringType()) \\\n",
    "      .add(\"arn\", StringType()) \\\n",
    "      .add(\"invokedBy\", StringType()) \\\n",
    "      .add(\"principalId\", StringType()) \\\n",
    "      .add(\"sessionContext\", StructType() \\\n",
    "        .add(\"attributes\", StructType() \\\n",
    "          .add(\"creationDate\", StringType()) \\\n",
    "          .add(\"mfaAuthenticated\", StringType()) \\\n",
    "        ) \\\n",
    "        .add(\"sessionIssuer\", StructType() \\\n",
    "          .add(\"accountId\", StringType()) \\\n",
    "          .add(\"arn\", StringType()) \\\n",
    "          .add(\"principalId\", StringType()) \\\n",
    "          .add(\"type\", StringType()) \\\n",
    "          .add(\"userName\", StringType()) \\\n",
    "        )\n",
    "      ) \\\n",
    "      .add(\"type\", StringType()) \\\n",
    "      .add(\"userName\", StringType()) \\\n",
    "      .add(\"webIdFederationData\", StructType() \\\n",
    "        .add(\"federatedProvider\", StringType()) \\\n",
    "        .add(\"attributes\", MapType(StringType(), StringType())) \\\n",
    "      ) \n",
    "    ) \\\n",
    "    .add(\"vpcEndpointId\", StringType())))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Only run this cell if you are creating the schema for the first time or you are usign a small data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fbda060af494c788cec7fc9b6f49620",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# read the records from S3\n",
    "#rawRecords = spark.read \\\n",
    "#  .json(cloudTrailLogsPath)\n",
    "# print the Schema\n",
    "#rawRecords.printSchema()\n",
    "# print the first 20 records\n",
    "#rawRecords.show()\n",
    "#\n",
    "#with open(\"s3n://aws-logs-994949494599-us-east-1/schema/cloudtrailschema.json\", \"w\") as f:\n",
    "#    json.dump(rawRecords.schema.jsonValue(), f)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Use this block if you have already created the schema file and saved it to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54a40fb1b47f4c1da130735b4085d795",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Records: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- additionalEventData: string (nullable = true)\n",
      " |    |    |-- apiVersion: string (nullable = true)\n",
      " |    |    |-- awsRegion: string (nullable = true)\n",
      " |    |    |-- errorCode: string (nullable = true)\n",
      " |    |    |-- errorMessage: string (nullable = true)\n",
      " |    |    |-- eventID: string (nullable = true)\n",
      " |    |    |-- eventName: string (nullable = true)\n",
      " |    |    |-- eventSource: string (nullable = true)\n",
      " |    |    |-- eventTime: string (nullable = true)\n",
      " |    |    |-- eventType: string (nullable = true)\n",
      " |    |    |-- eventVersion: string (nullable = true)\n",
      " |    |    |-- readOnly: boolean (nullable = true)\n",
      " |    |    |-- recipientAccountId: string (nullable = true)\n",
      " |    |    |-- requestID: string (nullable = true)\n",
      " |    |    |-- requestParameters: map (nullable = true)\n",
      " |    |    |    |-- key: string\n",
      " |    |    |    |-- value: string (valueContainsNull = true)\n",
      " |    |    |-- resources: array (nullable = true)\n",
      " |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |-- ARN: string (nullable = true)\n",
      " |    |    |    |    |-- accountId: string (nullable = true)\n",
      " |    |    |    |    |-- type: string (nullable = true)\n",
      " |    |    |-- responseElements: map (nullable = true)\n",
      " |    |    |    |-- key: string\n",
      " |    |    |    |-- value: string (valueContainsNull = true)\n",
      " |    |    |-- sharedEventID: string (nullable = true)\n",
      " |    |    |-- sourceIPAddress: string (nullable = true)\n",
      " |    |    |-- serviceEventDetails: map (nullable = true)\n",
      " |    |    |    |-- key: string\n",
      " |    |    |    |-- value: string (valueContainsNull = true)\n",
      " |    |    |-- userAgent: string (nullable = true)\n",
      " |    |    |-- userIdentity: struct (nullable = true)\n",
      " |    |    |    |-- accessKeyId: string (nullable = true)\n",
      " |    |    |    |-- accountId: string (nullable = true)\n",
      " |    |    |    |-- arn: string (nullable = true)\n",
      " |    |    |    |-- invokedBy: string (nullable = true)\n",
      " |    |    |    |-- principalId: string (nullable = true)\n",
      " |    |    |    |-- sessionContext: struct (nullable = true)\n",
      " |    |    |    |    |-- attributes: struct (nullable = true)\n",
      " |    |    |    |    |    |-- creationDate: string (nullable = true)\n",
      " |    |    |    |    |    |-- mfaAuthenticated: string (nullable = true)\n",
      " |    |    |    |    |-- sessionIssuer: struct (nullable = true)\n",
      " |    |    |    |    |    |-- accountId: string (nullable = true)\n",
      " |    |    |    |    |    |-- arn: string (nullable = true)\n",
      " |    |    |    |    |    |-- principalId: string (nullable = true)\n",
      " |    |    |    |    |    |-- type: string (nullable = true)\n",
      " |    |    |    |    |    |-- userName: string (nullable = true)\n",
      " |    |    |    |-- type: string (nullable = true)\n",
      " |    |    |    |-- userName: string (nullable = true)\n",
      " |    |    |    |-- webIdFederationData: struct (nullable = true)\n",
      " |    |    |    |    |-- federatedProvider: string (nullable = true)\n",
      " |    |    |    |    |-- attributes: map (nullable = true)\n",
      " |    |    |    |    |    |-- key: string\n",
      " |    |    |    |    |    |-- value: string (valueContainsNull = true)\n",
      " |    |    |-- vpcEndpointId: string (nullable = true)\n",
      "\n",
      "+--------------------+\n",
      "|             Records|\n",
      "+--------------------+\n",
      "|[[,, us-east-1, A...|\n",
      "|[[,, us-east-1,,,...|\n",
      "|[[,, us-east-1,,,...|\n",
      "|[[,, us-east-1,,,...|\n",
      "|[[,, us-east-1,,,...|\n",
      "|[[{\"SignatureVers...|\n",
      "|[[,, us-east-1, C...|\n",
      "|[[,, us-east-1, C...|\n",
      "|[[,, us-east-1,,,...|\n",
      "|[[,, us-east-1,,,...|\n",
      "|[[,, us-east-1,,,...|\n",
      "|[[,, us-east-1,,,...|\n",
      "|[[,, us-east-1,,,...|\n",
      "|[[,, us-east-1,,,...|\n",
      "|[[,, us-east-1,,,...|\n",
      "|[[,, us-east-1,,,...|\n",
      "|[[,, us-east-1,,,...|\n",
      "|[[,, us-east-1,,,...|\n",
      "|[[,, us-east-1,,,...|\n",
      "|[[,, us-east-1,,,...|\n",
      "+--------------------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "# Open the schema file\n",
    "#cloudTrailSchema = None\n",
    "#with open(\"s3n://aws-logs-994949494599-us-east-1/schema/cloudtrailschema.json\") as f:\n",
    "#    cloudTrailSchema = StructType.fromJson(json.load(f))\n",
    "#    print(cloudTrailSchema.simpleString())\n",
    "# read the records, should be much faster    \n",
    "rawRecords = spark.read \\\n",
    "  .option(\"maxFilesPerTrigger\", \"100\") \\\n",
    "  .schema(cloudTrailSchema) \\\n",
    "  .json(cloudTrailLogsPath)\n",
    "# print the Schema\n",
    "rawRecords.printSchema()\n",
    "# print the first 20 records\n",
    "rawRecords.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Now we get the raw JSON input and convert it to line type entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22052b542db2457c9633aa98182a9273",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- additionalEventData: string (nullable = true)\n",
      " |-- apiVersion: string (nullable = true)\n",
      " |-- awsRegion: string (nullable = true)\n",
      " |-- errorCode: string (nullable = true)\n",
      " |-- errorMessage: string (nullable = true)\n",
      " |-- eventID: string (nullable = true)\n",
      " |-- eventName: string (nullable = true)\n",
      " |-- eventSource: string (nullable = true)\n",
      " |-- eventTime: string (nullable = true)\n",
      " |-- eventType: string (nullable = true)\n",
      " |-- eventVersion: string (nullable = true)\n",
      " |-- readOnly: boolean (nullable = true)\n",
      " |-- recipientAccountId: string (nullable = true)\n",
      " |-- requestID: string (nullable = true)\n",
      " |-- requestParameters: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: string (valueContainsNull = true)\n",
      " |-- resources: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- ARN: string (nullable = true)\n",
      " |    |    |-- accountId: string (nullable = true)\n",
      " |    |    |-- type: string (nullable = true)\n",
      " |-- responseElements: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: string (valueContainsNull = true)\n",
      " |-- sharedEventID: string (nullable = true)\n",
      " |-- sourceIPAddress: string (nullable = true)\n",
      " |-- serviceEventDetails: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: string (valueContainsNull = true)\n",
      " |-- userAgent: string (nullable = true)\n",
      " |-- userIdentity: struct (nullable = true)\n",
      " |    |-- accessKeyId: string (nullable = true)\n",
      " |    |-- accountId: string (nullable = true)\n",
      " |    |-- arn: string (nullable = true)\n",
      " |    |-- invokedBy: string (nullable = true)\n",
      " |    |-- principalId: string (nullable = true)\n",
      " |    |-- sessionContext: struct (nullable = true)\n",
      " |    |    |-- attributes: struct (nullable = true)\n",
      " |    |    |    |-- creationDate: string (nullable = true)\n",
      " |    |    |    |-- mfaAuthenticated: string (nullable = true)\n",
      " |    |    |-- sessionIssuer: struct (nullable = true)\n",
      " |    |    |    |-- accountId: string (nullable = true)\n",
      " |    |    |    |-- arn: string (nullable = true)\n",
      " |    |    |    |-- principalId: string (nullable = true)\n",
      " |    |    |    |-- type: string (nullable = true)\n",
      " |    |    |    |-- userName: string (nullable = true)\n",
      " |    |-- type: string (nullable = true)\n",
      " |    |-- userName: string (nullable = true)\n",
      " |    |-- webIdFederationData: struct (nullable = true)\n",
      " |    |    |-- federatedProvider: string (nullable = true)\n",
      " |    |    |-- attributes: map (nullable = true)\n",
      " |    |    |    |-- key: string\n",
      " |    |    |    |-- value: string (valueContainsNull = true)\n",
      " |-- vpcEndpointId: string (nullable = true)\n",
      "\n",
      "+--------------------+----------+---------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+------------+--------+------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+-------------------+--------------------+--------------------+-------------+\n",
      "| additionalEventData|apiVersion|awsRegion|           errorCode|        errorMessage|             eventID|           eventName|         eventSource|           eventTime| eventType|eventVersion|readOnly|recipientAccountId|           requestID|   requestParameters|           resources|    responseElements|       sharedEventID|     sourceIPAddress|serviceEventDetails|           userAgent|        userIdentity|vpcEndpointId|\n",
      "+--------------------+----------+---------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+------------+--------+------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+-------------------+--------------------+--------------------+-------------+\n",
      "|                null|      null|us-east-1|        AccessDenied|User: arn:aws:sts...|4cf9d0af-5217-4a2...|           ListSteps|elasticmapreduce....|2020-12-16T15:16:56Z|AwsApiCall|        1.05|    null|      994949494599|5f08cf4d-01b8-4f3...|                null|                null|                null|                null|elasticmapreduce....|               null|elasticmapreduce....|[ASIA6PJ44O5DZZU5...|         null|\n",
      "|                null|      null|us-east-1|                null|                null|80c7d55f-0e06-492...|       ListInstances|elasticmapreduce....|2020-12-16T15:16:56Z|AwsApiCall|        1.05|    null|      994949494599|18537266-ff41-4d5...|[clusterId -> j-1...|                null|                null|                null|elasticmapreduce....|               null|elasticmapreduce....|[ASIA6PJ44O5DZZU5...|         null|\n",
      "|                null|      null|us-east-1|                null|                null|5460d8d8-e06b-417...|     DescribeCluster|elasticmapreduce....|2020-12-16T15:16:55Z|AwsApiCall|        1.05|    null|      994949494599|2692b989-28b9-46a...|[clusterId -> j-1...|                null|                null|                null|elasticmapreduce....|               null|elasticmapreduce....|[ASIA6PJ44O5DZZU5...|         null|\n",
      "|{\"SignatureVersio...|      null|us-east-1|ServerSideEncrypt...|The server side e...|3dfd080e-57e3-431...| GetBucketEncryption|    s3.amazonaws.com|2020-12-16T15:16:40Z|AwsApiCall|        1.05|    true|      994949494599|    6B4739AA2EEB710C|[bucketName -> aw...|[[arn:aws:s3:::aw...|                null|                null|        AWS Internal|               null|        AWS Internal|[ASIA6PJ44O5DZC36...|         null|\n",
      "|                null|      null|us-east-1|                null|                null|cd36a574-0ba9-44d...|   DescribeInstances|   ec2.amazonaws.com|2020-12-16T15:16:50Z|AwsApiCall|        1.05|    null|      994949494599|761e6a7e-f18c-40b...|[instancesSet -> ...|                null|                null|                null|elasticmapreduce....|               null|elasticmapreduce....|[, 994949494599, ...|         null|\n",
      "|                null|      null|us-east-1|Client.InvalidPer...|the specified rul...|58b8581a-2506-421...|AuthorizeSecurity...|   ec2.amazonaws.com|2020-12-16T15:16:57Z|AwsApiCall|        1.05|    null|      994949494599|e2fb92f2-91af-4cf...|[groupId -> sg-0f...|                null|                null|                null|elasticmapreduce....|               null|elasticmapreduce....|[, 994949494599, ...|         null|\n",
      "|                null|      null|us-east-1|                null|                null|b509f671-61bf-496...|          AssumeRole|   sts.amazonaws.com|2020-12-16T15:16:41Z|AwsApiCall|        1.05|    null|      994949494599|d0308fc8-7447-48a...|[roleArn -> arn:a...|[[arn:aws:iam::99...|[credentials -> {...|dcd2522b-f530-40c...|elasticmapreduce....|               null|elasticmapreduce....|[,,, elasticmapre...|         null|\n",
      "|                null|      null|us-east-1|                null|                null|5dfdf119-a0c2-436...|          AssumeRole|   sts.amazonaws.com|2020-12-16T15:16:44Z|AwsApiCall|        1.05|    null|      994949494599|3c2cb662-babc-4a1...|[roleArn -> arn:a...|[[arn:aws:iam::99...|[packedPolicySize...|07f0de91-6dc7-491...|elasticmapreduce....|               null|elasticmapreduce....|[,,, elasticmapre...|         null|\n",
      "|                null|      null|us-east-1|                null|                null|f63df090-486f-46c...|         ListEditors|elasticmapreduce....|2020-12-16T15:16:41Z|AwsApiCall|        1.05|    null|      994949494599|631c74ec-d3bf-471...|[listForUserOnly ...|                null|[editors -> [{\"ed...|                null|        72.21.198.67|               null|AWS ElasticMapRed...|[ASIA6PJ44O5DTTJ6...|         null|\n",
      "|                null|      null|us-east-1|                null|                null|7e2f42eb-ebc9-44b...|         ListEditors|elasticmapreduce....|2020-12-16T15:16:57Z|AwsApiCall|        1.05|    null|      994949494599|b45fc9e0-e52c-48a...|[listForUserOnly ...|                null|[editors -> [{\"ed...|                null|        72.21.198.67|               null|AWS ElasticMapRed...|[ASIA6PJ44O5DTTJ6...|         null|\n",
      "|                null|      null|us-east-1|                null|                null|96d831d2-c269-4ff...|          AssumeRole|   sts.amazonaws.com|2020-12-16T15:16:57Z|AwsApiCall|        1.05|    null|      994949494599|cef72cf2-039f-4aa...|[roleArn -> arn:a...|[[arn:aws:iam::99...|[credentials -> {...|cce34c72-07e9-4e8...|elasticmapreduce....|               null|elasticmapreduce....|[,,, elasticmapre...|         null|\n",
      "|                null|      null|us-east-1|                null|                null|97939115-e585-4b4...|          AssumeRole|   sts.amazonaws.com|2020-12-16T15:16:57Z|AwsApiCall|        1.05|    null|      994949494599|375c94f4-7f15-42b...|[roleArn -> arn:a...|[[arn:aws:iam::99...|[credentials -> {...|b210bde6-bfeb-4d9...|elasticmapreduce....|               null|elasticmapreduce....|[,,, elasticmapre...|         null|\n",
      "|                null|      null|us-east-1|                null|                null|a725e6ab-9731-4b4...|          AssumeRole|   sts.amazonaws.com|2020-12-16T15:16:57Z|AwsApiCall|        1.05|    null|      994949494599|b3dd1bab-2aa3-406...|[roleArn -> arn:a...|[[arn:aws:iam::99...|[packedPolicySize...|f4ba76e0-71fc-47f...|elasticmapreduce....|               null|elasticmapreduce....|[,,, elasticmapre...|         null|\n",
      "|                null|      null|us-east-1|                null|                null|18f5c574-e6f4-449...|    ListRepositories|elasticmapreduce....|2020-12-16T15:16:19Z|AwsApiCall|        1.05|    null|      994949494599|fe20ccb8-8d20-450...|[editorId -> e-60...|                null|[repositories -> []]|                null|        72.21.198.67|               null|AWS ElasticMapRed...|[ASIA6PJ44O5DTTJ6...|         null|\n",
      "|                null|      null|us-east-1|                null|                null|41eea487-c073-46b...|     DescribeSubnets|   ec2.amazonaws.com|2020-12-16T15:16:56Z|AwsApiCall|        1.05|    null|      994949494599|4e10b7ca-4413-4d7...|[subnetSet -> {\"i...|                null|                null|                null|elasticmapreduce....|               null|elasticmapreduce....|[, 994949494599, ...|         null|\n",
      "|                null|      null|us-east-1|                null|                null|3df552e7-5d64-48a...|   DescribeInstances|   ec2.amazonaws.com|2020-12-16T15:16:56Z|AwsApiCall|        1.05|    null|      994949494599|5e9d4075-c7fb-49a...|[instancesSet -> ...|                null|                null|                null|elasticmapreduce....|               null|elasticmapreduce....|[, 994949494599, ...|         null|\n",
      "|                null|      null|us-east-1|Client.InvalidGro...|The security grou...|66456b15-ca0b-429...| CreateSecurityGroup|   ec2.amazonaws.com|2020-12-16T15:16:57Z|AwsApiCall|        1.05|    null|      994949494599|6110d8d9-b199-486...|[groupName -> Ela...|                null|                null|                null|elasticmapreduce....|               null|elasticmapreduce....|[, 994949494599, ...|         null|\n",
      "|                null|      null|us-east-1|                null|                null|e0503083-0690-452...|DescribeSecurityG...|   ec2.amazonaws.com|2020-12-16T15:16:53Z|AwsApiCall|        1.05|    null|      994949494599|09c1fb9d-78a1-481...|[securityGroupSet...|                null|                null|                null|elasticmapreduce....|               null|elasticmapreduce....|[, 994949494599, ...|         null|\n",
      "|                null|      null|us-east-1|                null|                null|1f57770c-a483-420...|DescribeSecurityG...|   ec2.amazonaws.com|2020-12-16T15:16:53Z|AwsApiCall|        1.05|    null|      994949494599|686946df-e47d-410...|[securityGroupSet...|                null|                null|                null|elasticmapreduce....|               null|elasticmapreduce....|[, 994949494599, ...|         null|\n",
      "|                null|      null|us-east-1|                null|                null|f5a5261f-f483-44c...|CreateNetworkInte...|   ec2.amazonaws.com|2020-12-16T15:16:53Z|AwsApiCall|        1.05|    null|      994949494599|22ae14b2-144d-4f8...|[subnetId -> subn...|                null|[requestId -> 22a...|                null|elasticmapreduce....|               null|elasticmapreduce....|[, 994949494599, ...|         null|\n",
      "+--------------------+----------+---------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+------------+--------+------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+-------------------+--------------------+--------------------+-------------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "cloudTrailEvents = rawRecords \\\n",
    "  .select(explode(\"Records\").alias(\"record\")) \\\n",
    "  .select(\"record.*\")\n",
    "cloudTrailEvents.printSchema()\n",
    "cloudTrailEvents.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Check first item in the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b0eff67b68e4eedb8ed286fd61cac2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cloudtrailEvents first item: [Row(additionalEventData=None, apiVersion=None, awsRegion='us-east-1', errorCode='AccessDenied', errorMessage='User: arn:aws:sts::994949494599:assumed-role/EMR_Notebooks_DefaultRole/ElasticMapReduceEditorsSession is not authorized to perform: elasticmapreduce:ListSteps on resource: arn:aws:elasticmapreduce:us-east-1:994949494599:cluster/j-16C60UKKTQYOD', eventID='4cf9d0af-5217-4a23-ace2-bbb9c3298349', eventName='ListSteps', eventSource='elasticmapreduce.amazonaws.com', eventTime='2020-12-16T15:16:56Z', eventType='AwsApiCall', eventVersion='1.05', readOnly=None, recipientAccountId='994949494599', requestID='5f08cf4d-01b8-4f36-b9ab-0486ed898f15', requestParameters=None, resources=None, responseElements=None, sharedEventID=None, sourceIPAddress='elasticmapreduce.amazonaws.com', serviceEventDetails=None, userAgent='elasticmapreduce.amazonaws.com', userIdentity=Row(accessKeyId='ASIA6PJ44O5DZZU5I5BG', accountId='994949494599', arn='arn:aws:sts::994949494599:assumed-role/EMR_Notebooks_DefaultRole/ElasticMapReduceEditorsSession', invokedBy='elasticmapreduce.amazonaws.com', principalId='AROA6PJ44O5DWUCL5RD33:ElasticMapReduceEditorsSession', sessionContext=Row(attributes=Row(creationDate='2020-12-16T15:16:55Z', mfaAuthenticated='false'), sessionIssuer=Row(accountId='994949494599', arn='arn:aws:iam::994949494599:role/EMR_Notebooks_DefaultRole', principalId='AROA6PJ44O5DWUCL5RD33', type='Role', userName='EMR_Notebooks_DefaultRole')), type='AssumedRole', userName=None, webIdFederationData=None), vpcEndpointId=None)]"
     ]
    }
   ],
   "source": [
    "print('cloudtrailEvents first item: %s' % cloudTrailEvents.take(1))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Try a new method of creating the json rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a00545f185c4456b94ed05ffd5a91709",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['{\"awsRegion\":\"us-east-1\",\"errorCode\":\"AccessDenied\",\"errorMessage\":\"User: arn:aws:sts::994949494599:assumed-role/EMR_Notebooks_DefaultRole/ElasticMapReduceEditorsSession is not authorized to perform: elasticmapreduce:ListSteps on resource: arn:aws:elasticmapreduce:us-east-1:994949494599:cluster/j-16C60UKKTQYOD\",\"eventID\":\"4cf9d0af-5217-4a23-ace2-bbb9c3298349\",\"eventName\":\"ListSteps\",\"eventSource\":\"elasticmapreduce.amazonaws.com\",\"eventTime\":\"2020-12-16T15:16:56Z\",\"eventType\":\"AwsApiCall\",\"eventVersion\":\"1.05\",\"recipientAccountId\":\"994949494599\",\"requestID\":\"5f08cf4d-01b8-4f36-b9ab-0486ed898f15\",\"sourceIPAddress\":\"elasticmapreduce.amazonaws.com\",\"userAgent\":\"elasticmapreduce.amazonaws.com\",\"userIdentity\":{\"accessKeyId\":\"ASIA6PJ44O5DZZU5I5BG\",\"accountId\":\"994949494599\",\"arn\":\"arn:aws:sts::994949494599:assumed-role/EMR_Notebooks_DefaultRole/ElasticMapReduceEditorsSession\",\"invokedBy\":\"elasticmapreduce.amazonaws.com\",\"principalId\":\"AROA6PJ44O5DWUCL5RD33:ElasticMapReduceEditorsSession\",\"sessionContext\":{\"attributes\":{\"creationDate\":\"2020-12-16T15:16:55Z\",\"mfaAuthenticated\":\"false\"},\"sessionIssuer\":{\"accountId\":\"994949494599\",\"arn\":\"arn:aws:iam::994949494599:role/EMR_Notebooks_DefaultRole\",\"principalId\":\"AROA6PJ44O5DWUCL5RD33\",\"type\":\"Role\",\"userName\":\"EMR_Notebooks_DefaultRole\"}},\"type\":\"AssumedRole\"}}', '{\"awsRegion\":\"us-east-1\",\"eventID\":\"80c7d55f-0e06-492e-a71a-418b35e58ef3\",\"eventName\":\"ListInstances\",\"eventSource\":\"elasticmapreduce.amazonaws.com\",\"eventTime\":\"2020-12-16T15:16:56Z\",\"eventType\":\"AwsApiCall\",\"eventVersion\":\"1.05\",\"recipientAccountId\":\"994949494599\",\"requestID\":\"18537266-ff41-4d5d-a7ea-1578c927769c\",\"requestParameters\":{\"clusterId\":\"j-16C60UKKTQYOD\",\"instanceGroupTypes\":\"[\\\\\"MASTER\\\\\"]\"},\"sourceIPAddress\":\"elasticmapreduce.amazonaws.com\",\"userAgent\":\"elasticmapreduce.amazonaws.com\",\"userIdentity\":{\"accessKeyId\":\"ASIA6PJ44O5DZZU5I5BG\",\"accountId\":\"994949494599\",\"arn\":\"arn:aws:sts::994949494599:assumed-role/EMR_Notebooks_DefaultRole/ElasticMapReduceEditorsSession\",\"invokedBy\":\"elasticmapreduce.amazonaws.com\",\"principalId\":\"AROA6PJ44O5DWUCL5RD33:ElasticMapReduceEditorsSession\",\"sessionContext\":{\"attributes\":{\"creationDate\":\"2020-12-16T15:16:55Z\",\"mfaAuthenticated\":\"false\"},\"sessionIssuer\":{\"accountId\":\"994949494599\",\"arn\":\"arn:aws:iam::994949494599:role/EMR_Notebooks_DefaultRole\",\"principalId\":\"AROA6PJ44O5DWUCL5RD33\",\"type\":\"Role\",\"userName\":\"EMR_Notebooks_DefaultRole\"}},\"type\":\"AssumedRole\"}}']"
     ]
    }
   ],
   "source": [
    "import json\n",
    "#jsonCloudTrailEventsDF = cloudTrailEvents.toJSON().map(lambda j: json.loads(j)).collect()\n",
    "#jsonCloudTrailEventsDF = cloudTrailEvents.toJSON()\n",
    "jsonCloudTrailEventsRDD = cloudTrailEvents.toJSON()\n",
    "#jsonCloudTrailEventsDF.printSchema()\n",
    "# print the first 20 records\n",
    "#jsonCloudTrailEventsDF.show()\n",
    "jsonCloudTrailEventsRDD.take(2)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Add in the doc_id using a hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c84bfe7ee7ea4fdbb5e147bf7413678e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('4420bfb039aa6e059c76d267c6e07e3056e358d96675f32685aae7b6', '{\"awsRegion\": \"us-east-1\", \"errorCode\": \"AccessDenied\", \"errorMessage\": \"User: arn:aws:sts::994949494599:assumed-role/EMR_Notebooks_DefaultRole/ElasticMapReduceEditorsSession is not authorized to perform: elasticmapreduce:ListSteps on resource: arn:aws:elasticmapreduce:us-east-1:994949494599:cluster/j-16C60UKKTQYOD\", \"eventID\": \"4cf9d0af-5217-4a23-ace2-bbb9c3298349\", \"eventName\": \"ListSteps\", \"eventSource\": \"elasticmapreduce.amazonaws.com\", \"eventTime\": \"2020-12-16T15:16:56Z\", \"eventType\": \"AwsApiCall\", \"eventVersion\": \"1.05\", \"recipientAccountId\": \"994949494599\", \"requestID\": \"5f08cf4d-01b8-4f36-b9ab-0486ed898f15\", \"sourceIPAddress\": \"elasticmapreduce.amazonaws.com\", \"userAgent\": \"elasticmapreduce.amazonaws.com\", \"userIdentity\": {\"accessKeyId\": \"ASIA6PJ44O5DZZU5I5BG\", \"accountId\": \"994949494599\", \"arn\": \"arn:aws:sts::994949494599:assumed-role/EMR_Notebooks_DefaultRole/ElasticMapReduceEditorsSession\", \"invokedBy\": \"elasticmapreduce.amazonaws.com\", \"principalId\": \"AROA6PJ44O5DWUCL5RD33:ElasticMapReduceEditorsSession\", \"sessionContext\": {\"attributes\": {\"creationDate\": \"2020-12-16T15:16:55Z\", \"mfaAuthenticated\": \"false\"}, \"sessionIssuer\": {\"accountId\": \"994949494599\", \"arn\": \"arn:aws:iam::994949494599:role/EMR_Notebooks_DefaultRole\", \"principalId\": \"AROA6PJ44O5DWUCL5RD33\", \"type\": \"Role\", \"userName\": \"EMR_Notebooks_DefaultRole\"}}, \"type\": \"AssumedRole\"}}'), ('6d5564596f8b69454b46d2f751d81609e4f38806c9adb4810d1eb60e', '{\"awsRegion\": \"us-east-1\", \"eventID\": \"80c7d55f-0e06-492e-a71a-418b35e58ef3\", \"eventName\": \"ListInstances\", \"eventSource\": \"elasticmapreduce.amazonaws.com\", \"eventTime\": \"2020-12-16T15:16:56Z\", \"eventType\": \"AwsApiCall\", \"eventVersion\": \"1.05\", \"recipientAccountId\": \"994949494599\", \"requestID\": \"18537266-ff41-4d5d-a7ea-1578c927769c\", \"requestParameters\": {\"clusterId\": \"j-16C60UKKTQYOD\", \"instanceGroupTypes\": \"[\\\\\"MASTER\\\\\"]\"}, \"sourceIPAddress\": \"elasticmapreduce.amazonaws.com\", \"userAgent\": \"elasticmapreduce.amazonaws.com\", \"userIdentity\": {\"accessKeyId\": \"ASIA6PJ44O5DZZU5I5BG\", \"accountId\": \"994949494599\", \"arn\": \"arn:aws:sts::994949494599:assumed-role/EMR_Notebooks_DefaultRole/ElasticMapReduceEditorsSession\", \"invokedBy\": \"elasticmapreduce.amazonaws.com\", \"principalId\": \"AROA6PJ44O5DWUCL5RD33:ElasticMapReduceEditorsSession\", \"sessionContext\": {\"attributes\": {\"creationDate\": \"2020-12-16T15:16:55Z\", \"mfaAuthenticated\": \"false\"}, \"sessionIssuer\": {\"accountId\": \"994949494599\", \"arn\": \"arn:aws:iam::994949494599:role/EMR_Notebooks_DefaultRole\", \"principalId\": \"AROA6PJ44O5DWUCL5RD33\", \"type\": \"Role\", \"userName\": \"EMR_Notebooks_DefaultRole\"}}, \"type\": \"AssumedRole\"}}')]"
     ]
    }
   ],
   "source": [
    "import hashlib\n",
    "def f(row):\n",
    "    d = {}\n",
    "    rowjson = json.loads(row)\n",
    "    # removed the apiVersion because ElasticSearch will see it as a date instead of a string\n",
    "    if 'apiVersion' in rowjson:\n",
    "        rowjson.pop('apiVersion')\n",
    "    j=json.dumps(row).encode('ascii', 'ignore')\n",
    "    doc_id = hashlib.sha224(j).hexdigest()\n",
    "    row=json.dumps(rowjson)\n",
    "    return (doc_id, row)\n",
    "#Now populate that\n",
    "jsonCloudTrailEventsDF = jsonCloudTrailEventsRDD.map(f)\n",
    "jsonCloudTrailEventsDF.take(2)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Try turning it back into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cfbcff67d3f4a78be11f3c32c1aa017",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- doc_id: string (nullable = true)\n",
      " |-- event: string (nullable = true)\n",
      "\n",
      "+--------------------+--------------------+\n",
      "|              doc_id|               event|\n",
      "+--------------------+--------------------+\n",
      "|4420bfb039aa6e059...|{\"awsRegion\": \"us...|\n",
      "|6d5564596f8b69454...|{\"awsRegion\": \"us...|\n",
      "|151577f17388d20f0...|{\"awsRegion\": \"us...|\n",
      "|9968f156fec778404...|{\"additionalEvent...|\n",
      "|fdde8dbd426d55d74...|{\"awsRegion\": \"us...|\n",
      "|7e6b31d4ba2657141...|{\"awsRegion\": \"us...|\n",
      "|857459fbd5561c0ee...|{\"awsRegion\": \"us...|\n",
      "|aded0c553ebaa99a3...|{\"awsRegion\": \"us...|\n",
      "|9f96f3b0f017a303f...|{\"awsRegion\": \"us...|\n",
      "|78a64dd6b41fe43ba...|{\"awsRegion\": \"us...|\n",
      "|99107721387462868...|{\"awsRegion\": \"us...|\n",
      "|688ad8e8723da441c...|{\"awsRegion\": \"us...|\n",
      "|b0c9398646835b37b...|{\"awsRegion\": \"us...|\n",
      "|26e631b270514a570...|{\"awsRegion\": \"us...|\n",
      "|78c9eacfbe6edd031...|{\"awsRegion\": \"us...|\n",
      "|f0f2bdf0eff4f5f84...|{\"awsRegion\": \"us...|\n",
      "|abc80d1cceac0dce2...|{\"awsRegion\": \"us...|\n",
      "|22e460ac9f39254d8...|{\"awsRegion\": \"us...|\n",
      "|ff556ac5965b2234f...|{\"awsRegion\": \"us...|\n",
      "|8e084c70bbd3ddac4...|{\"awsRegion\": \"us...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "columns = [\"doc_id\",\"event\"]\n",
    "df = jsonCloudTrailEventsDF.toDF(columns)\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Get the user name and password for ElasticSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e6b29d06d0b4e74938749fc543b4920",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "name = \"YOUR USERNAME HERE\"\n",
    "password = \"YOUR PASSWORD HERE\"\n",
    "index = \"sdl/cloudtrail\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ac51abc2e94449f9aaf247fd48ffd67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.saveAsNewAPIHadoopFile.\n",
      ": org.apache.spark.SparkException: Job aborted.\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:100)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1083)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1.apply(PairRDDFunctions.scala:1081)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1.apply(PairRDDFunctions.scala:1081)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopDataset(PairRDDFunctions.scala:1081)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopFile$2.apply$mcV$sp(PairRDDFunctions.scala:1000)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopFile$2.apply(PairRDDFunctions.scala:991)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopFile$2.apply(PairRDDFunctions.scala:991)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:991)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.saveAsNewAPIHadoopFile(PythonRDD.scala:584)\n",
      "\tat org.apache.spark.api.python.PythonRDD.saveAsNewAPIHadoopFile(PythonRDD.scala)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 12 in stage 9.0 failed 4 times, most recent failure: Lost task 12.3 in stage 9.0 (TID 1127, ip-172-31-87-229.ec2.internal, executor 8): org.apache.spark.SparkException: Task failed while writing rows\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.org$apache$spark$internal$io$SparkHadoopWriter$$executeTask(SparkHadoopWriter.scala:157)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:83)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:78)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: org.elasticsearch.hadoop.rest.EsHadoopInvalidRequest: [HEAD] on [sdl] failed; server[search-dev-security-data-lake-gwgjiymvixomorzruasvi3i3ne.us-east-1.es.amazonaws.com:443] returned [500|Internal Server Error:]\n",
      "\tat org.elasticsearch.hadoop.rest.RestClient.checkResponse(RestClient.java:477)\n",
      "\tat org.elasticsearch.hadoop.rest.RestClient.executeNotFoundAllowed(RestClient.java:447)\n",
      "\tat org.elasticsearch.hadoop.rest.RestClient.exists(RestClient.java:539)\n",
      "\tat org.elasticsearch.hadoop.rest.RestClient.indexExists(RestClient.java:534)\n",
      "\tat org.elasticsearch.hadoop.rest.RestClient.touch(RestClient.java:545)\n",
      "\tat org.elasticsearch.hadoop.rest.RestRepository.touch(RestRepository.java:364)\n",
      "\tat org.elasticsearch.hadoop.rest.RestService.initSingleIndex(RestService.java:661)\n",
      "\tat org.elasticsearch.hadoop.rest.RestService.createWriter(RestService.java:637)\n",
      "\tat org.elasticsearch.hadoop.mr.EsOutputFormat$EsRecordWriter.init(EsOutputFormat.java:175)\n",
      "\tat org.elasticsearch.hadoop.mr.EsOutputFormat$EsRecordWriter.write(EsOutputFormat.java:150)\n",
      "\tat org.apache.spark.internal.io.HadoopMapReduceWriteConfigUtil.write(SparkHadoopWriter.scala:358)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$4.apply(SparkHadoopWriter.scala:132)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$4.apply(SparkHadoopWriter.scala:129)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1439)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.org$apache$spark$internal$io$SparkHadoopWriter$$executeTask(SparkHadoopWriter.scala:141)\n",
      "\t... 10 more\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:2043)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2031)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2030)\n",
      "\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2030)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:967)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:967)\n",
      "\tat scala.Option.foreach(Option.scala:257)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:967)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2264)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2213)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2202)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:778)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2114)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:78)\n",
      "\t... 27 more\n",
      "Caused by: org.apache.spark.SparkException: Task failed while writing rows\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.org$apache$spark$internal$io$SparkHadoopWriter$$executeTask(SparkHadoopWriter.scala:157)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:83)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:78)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\t... 1 more\n",
      "Caused by: org.elasticsearch.hadoop.rest.EsHadoopInvalidRequest: [HEAD] on [sdl] failed; server[search-dev-security-data-lake-gwgjiymvixomorzruasvi3i3ne.us-east-1.es.amazonaws.com:443] returned [500|Internal Server Error:]\n",
      "\tat org.elasticsearch.hadoop.rest.RestClient.checkResponse(RestClient.java:477)\n",
      "\tat org.elasticsearch.hadoop.rest.RestClient.executeNotFoundAllowed(RestClient.java:447)\n",
      "\tat org.elasticsearch.hadoop.rest.RestClient.exists(RestClient.java:539)\n",
      "\tat org.elasticsearch.hadoop.rest.RestClient.indexExists(RestClient.java:534)\n",
      "\tat org.elasticsearch.hadoop.rest.RestClient.touch(RestClient.java:545)\n",
      "\tat org.elasticsearch.hadoop.rest.RestRepository.touch(RestRepository.java:364)\n",
      "\tat org.elasticsearch.hadoop.rest.RestService.initSingleIndex(RestService.java:661)\n",
      "\tat org.elasticsearch.hadoop.rest.RestService.createWriter(RestService.java:637)\n",
      "\tat org.elasticsearch.hadoop.mr.EsOutputFormat$EsRecordWriter.init(EsOutputFormat.java:175)\n",
      "\tat org.elasticsearch.hadoop.mr.EsOutputFormat$EsRecordWriter.write(EsOutputFormat.java:150)\n",
      "\tat org.apache.spark.internal.io.HadoopMapReduceWriteConfigUtil.write(SparkHadoopWriter.scala:358)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$4.apply(SparkHadoopWriter.scala:132)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$4.apply(SparkHadoopWriter.scala:129)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1439)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.org$apache$spark$internal$io$SparkHadoopWriter$$executeTask(SparkHadoopWriter.scala:141)\n",
      "\t... 10 more\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 1438, in saveAsNewAPIHadoopFile\n",
      "    keyConverter, valueConverter, jconf)\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 63, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\", line 328, in get_return_value\n",
      "    format(target_id, \".\", name), value)\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.saveAsNewAPIHadoopFile.\n",
      ": org.apache.spark.SparkException: Job aborted.\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:100)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1083)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1.apply(PairRDDFunctions.scala:1081)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1.apply(PairRDDFunctions.scala:1081)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopDataset(PairRDDFunctions.scala:1081)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopFile$2.apply$mcV$sp(PairRDDFunctions.scala:1000)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopFile$2.apply(PairRDDFunctions.scala:991)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopFile$2.apply(PairRDDFunctions.scala:991)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:991)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.saveAsNewAPIHadoopFile(PythonRDD.scala:584)\n",
      "\tat org.apache.spark.api.python.PythonRDD.saveAsNewAPIHadoopFile(PythonRDD.scala)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 12 in stage 9.0 failed 4 times, most recent failure: Lost task 12.3 in stage 9.0 (TID 1127, ip-172-31-87-229.ec2.internal, executor 8): org.apache.spark.SparkException: Task failed while writing rows\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.org$apache$spark$internal$io$SparkHadoopWriter$$executeTask(SparkHadoopWriter.scala:157)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:83)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:78)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: org.elasticsearch.hadoop.rest.EsHadoopInvalidRequest: [HEAD] on [sdl] failed; server[search-dev-security-data-lake-gwgjiymvixomorzruasvi3i3ne.us-east-1.es.amazonaws.com:443] returned [500|Internal Server Error:]\n",
      "\tat org.elasticsearch.hadoop.rest.RestClient.checkResponse(RestClient.java:477)\n",
      "\tat org.elasticsearch.hadoop.rest.RestClient.executeNotFoundAllowed(RestClient.java:447)\n",
      "\tat org.elasticsearch.hadoop.rest.RestClient.exists(RestClient.java:539)\n",
      "\tat org.elasticsearch.hadoop.rest.RestClient.indexExists(RestClient.java:534)\n",
      "\tat org.elasticsearch.hadoop.rest.RestClient.touch(RestClient.java:545)\n",
      "\tat org.elasticsearch.hadoop.rest.RestRepository.touch(RestRepository.java:364)\n",
      "\tat org.elasticsearch.hadoop.rest.RestService.initSingleIndex(RestService.java:661)\n",
      "\tat org.elasticsearch.hadoop.rest.RestService.createWriter(RestService.java:637)\n",
      "\tat org.elasticsearch.hadoop.mr.EsOutputFormat$EsRecordWriter.init(EsOutputFormat.java:175)\n",
      "\tat org.elasticsearch.hadoop.mr.EsOutputFormat$EsRecordWriter.write(EsOutputFormat.java:150)\n",
      "\tat org.apache.spark.internal.io.HadoopMapReduceWriteConfigUtil.write(SparkHadoopWriter.scala:358)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$4.apply(SparkHadoopWriter.scala:132)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$4.apply(SparkHadoopWriter.scala:129)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1439)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.org$apache$spark$internal$io$SparkHadoopWriter$$executeTask(SparkHadoopWriter.scala:141)\n",
      "\t... 10 more\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:2043)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2031)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2030)\n",
      "\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2030)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:967)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:967)\n",
      "\tat scala.Option.foreach(Option.scala:257)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:967)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2264)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2213)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2202)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:778)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2114)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:78)\n",
      "\t... 27 more\n",
      "Caused by: org.apache.spark.SparkException: Task failed while writing rows\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.org$apache$spark$internal$io$SparkHadoopWriter$$executeTask(SparkHadoopWriter.scala:157)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:83)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:78)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\t... 1 more\n",
      "Caused by: org.elasticsearch.hadoop.rest.EsHadoopInvalidRequest: [HEAD] on [sdl] failed; server[search-dev-security-data-lake-gwgjiymvixomorzruasvi3i3ne.us-east-1.es.amazonaws.com:443] returned [500|Internal Server Error:]\n",
      "\tat org.elasticsearch.hadoop.rest.RestClient.checkResponse(RestClient.java:477)\n",
      "\tat org.elasticsearch.hadoop.rest.RestClient.executeNotFoundAllowed(RestClient.java:447)\n",
      "\tat org.elasticsearch.hadoop.rest.RestClient.exists(RestClient.java:539)\n",
      "\tat org.elasticsearch.hadoop.rest.RestClient.indexExists(RestClient.java:534)\n",
      "\tat org.elasticsearch.hadoop.rest.RestClient.touch(RestClient.java:545)\n",
      "\tat org.elasticsearch.hadoop.rest.RestRepository.touch(RestRepository.java:364)\n",
      "\tat org.elasticsearch.hadoop.rest.RestService.initSingleIndex(RestService.java:661)\n",
      "\tat org.elasticsearch.hadoop.rest.RestService.createWriter(RestService.java:637)\n",
      "\tat org.elasticsearch.hadoop.mr.EsOutputFormat$EsRecordWriter.init(EsOutputFormat.java:175)\n",
      "\tat org.elasticsearch.hadoop.mr.EsOutputFormat$EsRecordWriter.write(EsOutputFormat.java:150)\n",
      "\tat org.apache.spark.internal.io.HadoopMapReduceWriteConfigUtil.write(SparkHadoopWriter.scala:358)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$4.apply(SparkHadoopWriter.scala:132)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$4.apply(SparkHadoopWriter.scala:129)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1439)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.org$apache$spark$internal$io$SparkHadoopWriter$$executeTask(SparkHadoopWriter.scala:141)\n",
      "\t... 10 more\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "es_write_conf = {\n",
    "        \"es.nodes\" : \"search-dev-security-data-lake-gwgjiymvixomorzruasvi3i3ne.us-east-1.es.amazonaws.com\",\n",
    "        \"es.port\" : \"443\",\n",
    "        \"es.resource\" : index,\n",
    "        \"es.input.json\": \"yes\",\n",
    "        \"es.net.http.auth.user\": name,\n",
    "        \"es.net.http.auth.pass\": password,\n",
    "        \"es.net.ssl\": \"true\",\n",
    "        \"es.nodes.wan.only\": \"true\",\n",
    "        \"es.index.auto.create\": \"true\"\n",
    "    }\n",
    "jsonCloudTrailEventsDF.saveAsNewAPIHadoopFile(\n",
    "        path='-',\n",
    "        outputFormatClass=\"org.elasticsearch.hadoop.mr.EsOutputFormat\",       \n",
    "        keyClass=\"org.apache.hadoop.io.NullWritable\",\n",
    "        valueClass=\"org.elasticsearch.hadoop.mr.LinkedMapWritable\",\n",
    "        conf=es_write_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
